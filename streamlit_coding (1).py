# -*- coding: utf-8 -*-
"""Streamlit coding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12XdcPZF47_TwqYkJkL6HLM3wZ6lcKfJ8
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Lasso

# --------------------------------------------------
# PAGE CONFIG
# --------------------------------------------------
st.set_page_config(
    page_title="Predicting and Analysing Air Pollution Index in Malaysia",
    layout="wide"
)

st.title("üå´Ô∏è Air Pollution Index (API) Prediction App")
st.write("EDA, Model Development, Evaluation & Prediction")

# --------------------------------------------------
# LOAD DATA
# --------------------------------------------------
@st.cache_data
def load_data():
    return pd.read_csv("new_data.csv")

data = load_data()

# --------------------------------------------------
# PREPROCESSING & FEATURE ENGINEERING
# --------------------------------------------------
data = data.drop_duplicates()
data["air_pollution_concentration"].fillna(
    data["air_pollution_concentration"].median(), inplace=True
)

le = LabelEncoder()
data["air_pollutant_type"] = le.fit_transform(data["air_pollutant_type"])

limits = {
    "PM 2.5": 15,
    "PM 10": 45,
    "NO2": 0.02,
    "O3": 0.05,
    "CO": 4,
    "SO2": 0.02
}

pollutant_map = dict(zip(le.transform(le.classes_), le.classes_))

data["normalized_conc"] = data.apply(
    lambda row: row["air_pollution_concentration"] /
    limits[pollutant_map[row["air_pollutant_type"]]],
    axis=1
)

monthly_api = (
    data.groupby("month")["normalized_conc"]
    .mean()
    .reset_index()
    .rename(columns={"normalized_conc": "API_value"})
)

data = data.merge(monthly_api, on="month", how="left")

# Interaction features
data["traffic_emissions"] = data["car_registrations_y"] * data["normalized_conc"]
data["traffic_fire"] = data["car_registrations_y"] * data["fire_frp"]
data["ipi_pollution"] = data["ipi_index"] * data["normalized_conc"]
data["ipi_firefrp"] = data["ipi_index"] * data["fire_frp"]

features = [
    "avg_rainfall_mm",
    "fire_brightness",
    "fire_frp",
    "consumption",
    "normalized_conc",
    "traffic_emissions",
    "traffic_fire",
    "ipi_pollution",
    "ipi_firefrp"
]

X = data[features]
y = data["API_value"]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# --------------------------------------------------
# SIDEBAR MENU
# --------------------------------------------------
menu = st.sidebar.selectbox(
    "Select Section",
    [
        "EDA",
        "Model Development (with Tuning)",
        "Model Evaluation",
        "Model Comparison",
        "Best Model & Prediction"
    ]
)

# --------------------------------------------------
# EDA
# --------------------------------------------------
if menu == "EDA":
    st.subheader("üìä Exploratory Data Analysis")

    numeric_cols = data.select_dtypes(include=np.number).columns
    feature = st.selectbox("Select feature", numeric_cols)

    fig, ax = plt.subplots()
    ax.hist(data[feature], bins=30)
    ax.set_title(f"Distribution of {feature}")
    st.pyplot(fig)

    st.dataframe(data[numeric_cols].describe())

# --------------------------------------------------
# MODEL DEVELOPMENT (WITH TUNING)
# --------------------------------------------------
if menu == "Model Development (with Tuning)":
    st.subheader("‚öôÔ∏è Model Development & Hyperparameter Tuning")

    model_choice = st.selectbox(
        "Select Model",
        ["Decision Tree (Tuned)", "Random Forest", "Gradient Boosting", "Linear Regression (Tuned)"]
    )

    if model_choice == "Decision Tree (Tuned)":
        param_grid = {
            "max_depth": [None, 5, 10, 15],
            "min_samples_split": [2, 5, 10],
            "min_samples_leaf": [1, 2, 4]
        }

        grid = GridSearchCV(
            DecisionTreeRegressor(random_state=42),
            param_grid,
            cv=5,
            scoring="r2",
            n_jobs=-1
        )

        grid.fit(X_train, y_train)
        model = grid.best_estimator_
        st.write("Best Parameters:", grid.best_params_)
        y_pred = model.predict(X_test)

    elif model_choice == "Random Forest":
        model = RandomForestRegressor(n_estimators=200, random_state=42)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

    elif model_choice == "Gradient Boosting":
        model = GradientBoostingRegressor(random_state=42)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

    else:
        poly = PolynomialFeatures(degree=3)
        X_poly_train = poly.fit_transform(X_train)
        X_poly_test = poly.transform(X_test)

        grid = GridSearchCV(
            Lasso(max_iter=3000),
            {"alpha": [0.001, 0.01, 0.1, 1, 10]},
            cv=5,
            scoring="r2"
        )

        grid.fit(X_poly_train, y_train)
        model = grid.best_estimator_
        st.write("Best Alpha:", grid.best_params_["alpha"])
        y_pred = model.predict(X_poly_test)

    fig, ax = plt.subplots()
    ax.scatter(y_test, y_pred, alpha=0.6)
    ax.plot([y_test.min(), y_test.max()],
            [y_test.min(), y_test.max()], "r--")
    ax.set_xlabel("Actual")
    ax.set_ylabel("Predicted")
    ax.set_title("Actual vs Predicted")
    st.pyplot(fig)

# --------------------------------------------------
# MODEL EVALUATION
# --------------------------------------------------
if menu == "Model Evaluation":
    st.subheader("üìà Model Evaluation")

    models = {
        "Decision Tree": DecisionTreeRegressor(random_state=42),
        "Random Forest": RandomForestRegressor(n_estimators=200, random_state=42),
        "Gradient Boosting": GradientBoostingRegressor(random_state=42),
        "Linear Regression": LinearRegression()
    }

    results = []

    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        results.append({
            "Model": name,
            "MAE": mean_absolute_error(y_test, y_pred),
            "RMSE": np.sqrt(mean_squared_error(y_test, y_pred)),
            "R2": r2_score(y_test, y_pred),
            "CV R2 Mean": cross_val_score(model, X_train, y_train, cv=5, scoring="r2").mean()
        })

    results_df = pd.DataFrame(results)
    st.dataframe(results_df)

# --------------------------------------------------
# MODEL COMPARISON
# --------------------------------------------------
if menu == "Model Comparison":
    st.subheader("üèÜ Model Comparison")

    metric = st.selectbox("Select metric", ["R2", "RMSE", "MAE", "CV R2 Mean"])

    models = {
        "Decision Tree": DecisionTreeRegressor(random_state=42),
        "Random Forest": RandomForestRegressor(n_estimators=200, random_state=42),
        "Gradient Boosting": GradientBoostingRegressor(random_state=42),
        "Linear Regression": LinearRegression()
    }

    comparison = []

    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        comparison.append({
            "Model": name,
            "MAE": mean_absolute_error(y_test, y_pred),
            "RMSE": np.sqrt(mean_squared_error(y_test, y_pred)),
            "R2": r2_score(y_test, y_pred),
            "CV R2 Mean": cross_val_score(model, X_train, y_train, cv=5, scoring="r2").mean()
        })

    df = pd.DataFrame(comparison)

    fig, ax = plt.subplots()
    ax.bar(df["Model"], df[metric])
    ax.set_title(f"Model Comparison based on {metric}")
    st.pyplot(fig)

# --------------------------------------------------
# BEST MODEL & PREDICTION
# --------------------------------------------------
if menu == "Best Model & Prediction":
    st.subheader("‚úÖ Best Model: Gradient Boosting")

    gb_model = GradientBoostingRegressor(random_state=42)
    gb_model.fit(X_train, y_train)

    st.subheader("üîÆ Predict API Index")

    user_inputs = [st.number_input(f, value=float(data[f].mean())) for f in features]

    input_df = pd.DataFrame([user_inputs], columns=features)
    input_scaled = scaler.transform(input_df)

    prediction = gb_model.predict(input_scaled)[0]
    st.success(f"### Predicted API Value: **{prediction:.4f}**")