# -*- coding: utf-8 -*-
"""Streamlit coding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12XdcPZF47_TwqYkJkL6HLM3wZ6lcKfJ8
"""

import streamlit as st
import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# --------------------------------------------------
# PAGE CONFIG
# --------------------------------------------------
st.set_page_config(
    page_title="Air Pollution API Prediction",
    layout="wide"
)

st.title("üå´Ô∏è Air Pollution Index (API) Prediction App")
st.write("Deployed using Gradient Boosting Regression")

# --------------------------------------------------
# LOAD DATA
# --------------------------------------------------
@st.cache_data
def load_data():
    return pd.read_csv("new_data.csv")

data = load_data()

# --------------------------------------------------
# PREPROCESSING
# --------------------------------------------------
data = data.drop_duplicates()

# Handle missing values
data["air_pollution_concentration"].fillna(
    data["air_pollution_concentration"].median(), inplace=True
)

# Label Encoding
le = LabelEncoder()
data["air_pollutant_type"] = le.fit_transform(data["air_pollutant_type"])

# Pollution limits
limits = {
    "PM 2.5": 15,
    "PM 10": 45,
    "NO2": 0.02,
    "O3": 0.05,
    "CO": 4,
    "SO2": 0.02
}

# Reverse map for normalization
pollutant_map = dict(zip(le.transform(le.classes_), le.classes_))

data["normalized_conc"] = data.apply(
    lambda row: row["air_pollution_concentration"] /
    limits[pollutant_map[row["air_pollutant_type"]]],
    axis=1
)

# Monthly API
monthly_api = (
    data.groupby("month")["normalized_conc"]
    .mean()
    .reset_index()
    .rename(columns={"normalized_conc": "API_value"})
)

data = data.merge(monthly_api, on="month", how="left")

# --------------------------------------------------
# FEATURE ENGINEERING
# --------------------------------------------------
data["traffic_emissions"] = data["car_registrations_y"] * data["normalized_conc"]
data["traffic_fire"] = data["car_registrations_y"] * data["fire_frp"]
data["ipi_pollution"] = data["ipi_index"] * data["normalized_conc"]
data["ipi_firefrp"] = data["ipi_index"] * data["fire_frp"]

# Target & features
y = data["API_value"]
X = data[
    [
        "avg_rainfall_mm",
        "fire_brightness",
        "fire_frp",
        "consumption",
        "normalized_conc",
        "traffic_emissions",
        "traffic_fire",
        "ipi_pollution",
        "ipi_firefrp"
    ]
]

# Scale
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# --------------------------------------------------
# MODEL TRAINING
# --------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

model = GradientBoostingRegressor(random_state=42)
model.fit(X_train, y_train)

# --------------------------------------------------
# MODEL PERFORMANCE
# --------------------------------------------------
y_pred = model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

st.subheader("üìä Model Performance")
st.metric("RMSE", f"{rmse:.4f}")
st.metric("R¬≤ Score", f"{r2:.4f}")

# --------------------------------------------------
# USER INPUT PREDICTION
# --------------------------------------------------
st.subheader("üîÆ Predict API Value")

col1, col2, col3 = st.columns(3)

avg_rainfall_mm = col1.number_input("Average Rainfall (mm)", value=9.0)
fire_brightness = col1.number_input("Fire Brightness", value=330.0)
fire_frp = col1.number_input("Fire FRP", value=1.6)

consumption = col2.number_input("Energy Consumption", value=14000.0)
car_registrations_y = col2.number_input("Car Registrations", value=300.0)
ipi_index = col2.number_input("IPI Index", value=-0.1)